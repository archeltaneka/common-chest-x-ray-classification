{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dissertation Project Code: Common Chest X-ray Classification and Localization with Deep Learning"},{"metadata":{},"cell_type":"markdown","source":"This is another source code for my final project dissertation on \"Chest X-ray Classification and Localization\" by analysing (breaking down) each disease"},{"metadata":{},"cell_type":"markdown","source":"Let's start by importing the libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom glob import glob\nfrom matplotlib.patches import Rectangle\n\nimport keras.backend as K\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, image\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.applications import VGG16, VGG19, MobileNet, MobileNetV2, InceptionResNetV2, InceptionV3, ResNet50, DenseNet121, DenseNet169, DenseNet201\nfrom keras import regularizers, optimizers\nfrom keras.applications.vgg16 import decode_predictions, preprocess_input\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input/data')) # list the items in the directory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't forget to create the glob object and insert the full path to the directory of the image to a new column."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/data/images*/images/*.png'\npattern = glob(PATH)\nprint(\"Total number of images: \", len(pattern))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\ndata = pd.read_csv('../input/data/Data_Entry_2017.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_path = {os.path.basename(x): x for x in pattern}\ndata['full_path'] = data['Image Index'].map(full_path.get)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Breaking Down The Diseases"},{"metadata":{},"cell_type":"markdown","source":"To simplify the problem, let's take the 1 vs all approach"},{"metadata":{},"cell_type":"markdown","source":"Let's take \"Infiltration\" for example. We can try with different diseases by commenting and uncommenting the corresponding codes below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# data['is_infiltration'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Infiltration' in result else 'No')\n# data['is_atelectasis'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Atelectasis' in result else 'No')\n# data['is_cardiomegaly'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Cardiomegaly' in result else 'No')\n# data['is_mass'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Mass' in result else 'No')\n# data['is_nodule'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Nodule' in result else 'No')\n# data['is_pneumonia'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Pneumonia' in result else 'No')\n# data['is_consolidation'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Consolidation' in result else 'No')\n# data['is_edema'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Edema' in result else 'No')\n# data['is_fibrosis'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Fibrosis' in result else 'No')\n# data['is_effusion'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Effusion' in result else 'No')\n# data['is_pleural'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Pleural_Thickening' in result else 'No')\n# data['is_hernia'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Hernia' in result else 'No')\n# data['is_emphysema'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Emphysema' in result else 'No')\ndata['is_pneumothorax'] = data['Finding Labels'].map(lambda result: 'Yes' if 'Pneumothorax' in result else 'No')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of using the one-hot encoding method to create a label for the dataset, **we can just create a label if there is the word \"Infiltration\" in the \"Finding Labels\" column**<br>\n\nNow the problem becomes a binary classification problem, instead of the multi classification problem which makes it easier for us"},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing and Augmentation"},{"metadata":{},"cell_type":"markdown","source":"Split between train/test/val data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the training and validation set\ntrain_set, val_set = train_test_split(data, test_size=0.1, random_state=1993)\ntrain_set, test_set = train_test_split(train_set, test_size=0.1, random_state=1993)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same data augmentation as before"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creates image data generator to improve x-ray readings\nIMG_SIZE = (224, 224)\ntrain_datagen = ImageDataGenerator(rescale=1./255, # scales the image pixel value\n                                   #samplewise_center=True,\n                                   horizontal_flip=True, # allows horizontal flip\n                                   vertical_flip=False, # we don't want the xrays to be upside-down\n                                   height_shift_range=0.2,\n                                   width_shift_range=0.2,\n                                   rotation_range=20, # random rotations to 20 degrees\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   )\n\ntest_datagen = ImageDataGenerator(rescale=1./255) # just scale the pixel image, we don't want to preprocess the test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I just found that Keras has built-in method called **.flow_from_dataframe** which is the same as **flow_from_dataframe** method from the previous notebook.<br><br>\n\nLet's try it in this notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator = train_datagen.flow_from_dataframe(dataframe=train_set,\n                                                   directory=None,\n                                                   batch_size=32,\n                                                    x_col='full_path',\n                                                    y_col='is_pneumothorax',\n                                                    shuffle=True,\n                                                    target_size=IMG_SIZE,\n                                                    seed=111,\n                                                   class_mode='binary')\n\nval_X, val_Y = next(test_datagen.flow_from_dataframe(dataframe=val_set,\n                                                   directory=None,\n                                                   batch_size=256,\n                                                   x_col='full_path',\n                                                   y_col='is_pneumothorax',\n                                                   shuffle=True,\n                                                   target_size=IMG_SIZE,\n                                                   seed=111,\n                                                   class_mode='binary'))\n\ntest_X, test_Y = next(test_datagen.flow_from_dataframe(dataframe=test_set,\n                                                   directory=None,\n                                                   batch_size=1024,\n                                                   x_col='full_path',\n                                                   y_col='is_pneumothorax',\n                                                   shuffle=True,\n                                                   target_size=IMG_SIZE,\n                                                   seed=111,\n                                                   class_mode='binary'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't forget to **change class_mode into binary** instead of sparse or categorical, since now we are dealing with the binary classification problem"},{"metadata":{},"cell_type":"markdown","source":"## Building The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_choice = 'VGG16'\n# VGG16\nif base_choice.upper() == 'VGG16':\n    base_model = VGG16(include_top=False, input_shape=(224,224,3), weights='imagenet')\n# VGG19\nelif base_choice.upper() == 'VGG19':   \n    base_model = VGG19(include_top=False, input_shape=(224,224,3))\n# MobileNet\nelif base_choice.upper() == 'MOBILE':\n    base_model = MobileNet(include_top=False, input_shape=(224,224,3))\n# MobileNetV2\nelif base_choice.upper() == 'MOBILEV2':\n    base_model = MobileNetV2(include_top=False, input_shape=(224,224,3))\n# InceptionResNetV2\nelif base_choice.upper() == 'INCEPTIONV2':\n    base_model = InceptionResNetV2(include_top=False, input_shape=(224,224,3))\n# InceptionV3\nelif base_choice.upper() == 'INCEPTIONV3':\n    base_model = InceptionV3(include_top=False, input_shape=(224,224,3))\n# ResNet50\nelif base_choice.upper() == 'RESNET50':\n    base_model = ResNet50(include_top=False, input_shape=(224,224,3))\n# DenseNet 121\nelif base_choice.upper() == 'DENSE121':\n    base_model = DenseNet121(include_top=False, input_shape=(224,224,3))\n# DenseNet 169\nelif base_choice.upper() == 'DENSE169':\n    base_model = DenseNet169(include_top=False, input_shape=(224,224,3))\n# DenseNet 201\nelif base_choice.upper() == 'DENSE201':\n    base_model = DenseNet201(include_top=False, input_shape=(224,224,3))\n    \nprint(\"Base pre-trained model:\", base_choice)\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# freeze the base model\nfor layer in base_model.layers:\n    layer.trainable = False\n        \n# adds our own dense layers\noutput = base_model.output\noutput = Flatten()(output)\noutput = Dense(512, activation='relu', kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=regularizers.l2(0.01))(output)\nlast_output = Dense(1, activation='sigmoid')(output)\n# construct final model\nfinal_model = Model(base_model.input, last_output)\n# compile the model\n# opt = optimizers.Adam(lr=1e-3, decay=1e-5)\nfinal_model.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['binary_accuracy'])\nfinal_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saves the best model for each epoch\n# WEIGHT_PATH = 'checkpoint.{epoch:02d}-{val_loss:.2f}.hdf5'\n# checkpoint = ModelCheckpoint(filepath=WEIGHT_PATH, monitor='val_loss', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_model = final_model.fit_generator(generator=train_generator, steps_per_epoch=len(train_generator)//32, epochs=5, validation_data=(val_X, val_Y), validation_steps=len(val_X)//256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.save('weights.'+base_choice+ '-' + 'pneumothorax' + '.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_Y = final_model.predict(test_X, batch_size = 64, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.evaluate(test_X, test_Y, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see our ROC is only consisted of one curve, which is the disease we broke down at the beginning of this notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n# for (idx, c_label) in enumerate(diseases):\nfpr, tpr, thresholds = roc_curve(test_Y.astype(int), pred_Y)\nc_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % ('Infiltration', auc(fpr, tpr)))\nc_ax.legend()\nc_ax.set_xlabel('False Positive Rate')\nc_ax.set_ylabel('True Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fitted_model.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(fitted_model.history['loss'])\nplt.plot(fitted_model.history['val_loss'])\nplt.title('Model Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(fitted_model.history['binary_accuracy'])\nplt.plot(fitted_model.history['val_binary_accuracy'])\nplt.title('Training Accuracy vs validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this directory, there is also a data for the correct localisation from several images. It is in the BBox_List_2017.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"box_data = pd.read_csv('../input/data/BBox_List_2017.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(box_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember that this is only consists of 984 data, not the whole 112,120 data that we used to train and test the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"box_data['Finding Label'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, there are only 8 diseases available in this dataset"},{"metadata":{},"cell_type":"markdown","source":"Let's focus on Infiltration first"},{"metadata":{"trusted":true},"cell_type":"code","source":"infiltrate = box_data[box_data['Finding Label'] == 'Infiltrate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"infiltrate.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4 columns (x, y, w, and h) that we can use to draw a rectangle to locate where the disease is in the image."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(data)):\n    if data['Image Index'].iloc[i] == '00025787_027.png':\n        print(data['full_path'].iloc[i])\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test with a random x-ray image which has the Infiltration disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_IMG = '../input/data/images_011/images/00025787_027.png'\n\nimg = load_img(PATH_IMG, target_size=(224,224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a prediction with our model on that image"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = final_model.predict(x) # predict with the model we just trained\npred = list(pred.flatten())\nargmax = np.argmax(pred[0])\noutput = final_model.output[:, argmax]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Localisation with Grad-CAM"},{"metadata":{},"cell_type":"markdown","source":"Grad-CAM is similar with the CAM algorithm. However, it implements gradient-weighted class activation in the process."},{"metadata":{},"cell_type":"markdown","source":"> Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. (R. Ramprasaath et al., 2019)."},{"metadata":{},"cell_type":"markdown","source":"The general algorithm looks like this:\n\n1. It extracts the final convolutional layer from the model like CAM does\n2. The next step is slightly different from CAM. Instead of just taking the logits (np.argmax) from the class, it also extracts the gradients of that class.\n3. Next, take the mean from it. Why mean? This method does not need us to add another pooling layer e.g. GlobalAveragePooling2D.\n4. Construct a temporary model with the input from final_model and output from the last convolutional layer output"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take the last convolutional layer\nlast_conv_layer = final_model.get_layer('block5_conv3')\n# extract the gradients from the last convolutional layer against the class logits (np.argmax)\ngrads = K.gradients(output, last_conv_layer.output)[0]\n# print(grads.shape)\n# take the mean of the gradients, leaving us with the channel dimension --> global average pooling\npooled_grads = K.mean(grads, axis=(0,1,2))\n# define a temporary model with pre-trained model as its input and the last convolutional layer as its output\ni = K.function([final_model.input], [pooled_grads, last_conv_layer.output[0]])\npooled_grads_value, conv_layer_output_value = i([x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Finally, compute the matrix multiplication from both of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# do the matrix multiplication to obtain weight between the last convolutional layer and its gradient\nfor l in range(512):\n    conv_layer_output_value[:, :, l] *= pooled_grads_value[l]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't forget to normalize those values into 0-1 range"},{"metadata":{"trusted":true},"cell_type":"code","source":"# average the weighted feature map along the channel dimension which resulting in a heatmap\nheatmap = np.mean(conv_layer_output_value, axis=-1)\n# normalize the heatmap\nheatmap = np.maximum(heatmap, 0)\nheatmap /= np.max(heatmap)\n# display the matrix values with matshow\nplt.matshow(heatmap) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can get a glimpse of where the the disease occured (if any) just by seeing the plot above."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's apply this heatmap to our test image"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread(PATH_IMG)\nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n# multiply heatmap by 255 to convert it back to RGB\nheatmap = np.uint8(255 * heatmap)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\nhif = .5 # intensity (transparency) value\n\nsuperimposed_img = heatmap * hif + img # paste the heatmap * intensity to the original image\noutput = 'output.jpeg'\ncv2.imwrite(output, superimposed_img)\nimg=mpimg.imread(output)\nplt.imshow(img)\nplt.axis('off')\nplt.title('Result:' + str('Normal' if pred[0] < 0.5 else 'Infiltration') + '|' + 'Confidence:' + str(pred[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we are done! :)"},{"metadata":{},"cell_type":"markdown","source":"We can check if our localisation is the same with the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get x,y,w,h from the row of the test image\nrow = box_data[box_data['Image Index'] == '00025787_027.png']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row.iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row['Bbox [x'].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# draw a rectangle around the chest x-ray\nplt.imshow(cv2.imread(PATH_IMG))\nrect = Rectangle((row['Bbox [x'].iloc[1],row['y'].iloc[1]), \n                  row['w'].iloc[1], row['h]'].iloc[1], \n                  fill=False, color='red')\nplt.axes().add_patch(rect)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}