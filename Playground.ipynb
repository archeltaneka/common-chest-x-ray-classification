{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Playground! I made this notebook to play around while completing the convolutional neural network algorithm from scratch.<br> \n",
    "I recommend you to also play around with this notebook to understand how the algorithm works. Feel free to experiment by changing the codes.\n",
    "<br><br>\n",
    "This notebook is based on:\n",
    "- https://victorzhou.com/blog/intro-to-cnns-part-1/\n",
    "- https://victorzhou.com/blog/intro-to-cnns-part-2/<br>\n",
    "<br>Have a look at these links for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple MNIST handwritten digits dataset. Tensorflow has mnist dataset built in the library, just import them and it will automatically split the data into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use simple data with mnist digits\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is a grayscale image, which means it has only 1 color channel for each image unlike images in general with 3 color channels (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first image inside the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ca5264fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADgpJREFUeJzt3X+MVfWZx/HPs1j+kKI4aQRCYSnEYJW4082IjSWrxkzVDQZHrekkJjQapn8wiU02ZA3/VNNgyCrslmiamaZYSFpKE3VB0iw0otLGZuKIWC0srTFsO3IDNTjywx9kmGf/mEMzxbnfe+fec++5zPN+JeT+eM6558kNnznn3O+592vuLgDx/EPRDQAoBuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUZc3cmJlxOSHQYO5u1SxX157fzO40syNm9q6ZPVrPawFoLqv12n4zmybpj5I6JQ1Jel1St7sfSqzDnh9osGbs+ZdJetfd33P3c5J+IWllHa8HoInqCf88SX8Z93goe+7vmFmPmQ2a2WAd2wKQs3o+8Jvo0OJzh/Xu3i+pX+KwH2gl9ez5hyTNH/f4y5KO1dcOgGapJ/yvS7rGzL5iZtMlfVvSrnzaAtBoNR/2u/uImfVK2iNpmqQt7v6H3DoD0FA1D/XVtDHO+YGGa8pFPgAuXYQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfMU3ZJkZkclnZZ0XtKIu3fk0RTyM23atGT9yiuvbOj2e3t7y9Yuv/zy5LpLlixJ1tesWZOsP/XUU2Vr3d3dyXU//fTTZH3Dhg3J+uOPP56st4K6wp+5zd0/yOF1ADQRh/1AUPWG3yXtNbM3zKwnj4YANEe9h/3fcPdjZna1pF+b2f+6+/7xC2R/FPjDALSYuvb87n4suz0h6QVJyyZYpt/dO/gwEGgtNYffzGaY2cwL9yV9U9I7eTUGoLHqOeyfLekFM7vwOj939//JpSsADVdz+N39PUn/lGMvU9aCBQuS9enTpyfrN998c7K+fPnysrVZs2Yl173vvvuS9SINDQ0l65s3b07Wu7q6ytZOnz6dXPett95K1l999dVk/VLAUB8QFOEHgiL8QFCEHwiK8ANBEX4gKHP35m3MrHkba6L29vZkfd++fcl6o79W26pGR0eT9YceeihZP3PmTM3bLpVKyfqHH36YrB85cqTmbTeau1s1y7HnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfPQVtbW7I+MDCQrC9atCjPdnJVqffh4eFk/bbbbitbO3fuXHLdqNc/1ItxfgBJhB8IivADQRF+ICjCDwRF+IGgCD8QVB6z9IZ38uTJZH3t2rXJ+ooVK5L1N998M1mv9BPWKQcPHkzWOzs7k/WzZ88m69dff33Z2iOPPJJcF43Fnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX4z2yJphaQT7r40e65N0g5JCyUdlfSAu6d/6FxT9/v89briiiuS9UrTSff19ZWtPfzww8l1H3zwwWR9+/btyTpaT57f5/+ppDsveu5RSS+5+zWSXsoeA7iEVAy/u++XdPElbCslbc3ub5V0T859AWiwWs/5Z7t7SZKy26vzawlAMzT82n4z65HU0+jtAJicWvf8x81sriRltyfKLeju/e7e4e4dNW4LQAPUGv5dklZl91dJ2plPOwCapWL4zWy7pN9JWmJmQ2b2sKQNkjrN7E+SOrPHAC4hFc/53b27TOn2nHsJ69SpU3Wt/9FHH9W87urVq5P1HTt2JOujo6M1bxvF4go/ICjCDwRF+IGgCD8QFOEHgiL8QFBM0T0FzJgxo2ztxRdfTK57yy23JOt33XVXsr53795kHc3HFN0Akgg/EBThB4Ii/EBQhB8IivADQRF+ICjG+ae4xYsXJ+sHDhxI1oeHh5P1l19+OVkfHBwsW3vmmWeS6zbz/+ZUwjg/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf7gurq6kvVnn302WZ85c2bN2163bl2yvm3btmS9VCrVvO2pjHF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBUxXF+M9siaYWkE+6+NHvuMUmrJf01W2ydu/+q4sYY57/kLF26NFnftGlTsn777bXP5N7X15esr1+/Pll///33a972pSzPcf6fSrpzguf/093bs38Vgw+gtVQMv7vvl3SyCb0AaKJ6zvl7zez3ZrbFzK7KrSMATVFr+H8kabGkdkklSRvLLWhmPWY2aGblf8wNQNPVFH53P+7u5919VNKPJS1LLNvv7h3u3lFrkwDyV1P4zWzuuIddkt7Jpx0AzXJZpQXMbLukWyV9ycyGJH1f0q1m1i7JJR2V9N0G9gigAfg+P+oya9asZP3uu+8uW6v0WwFm6eHqffv2JeudnZ3J+lTF9/kBJBF+ICjCDwRF+IGgCD8QFOEHgmKoD4X57LPPkvXLLktfhjIyMpKs33HHHWVrr7zySnLdSxlDfQCSCD8QFOEHgiL8QFCEHwiK8ANBEX4gqIrf50dsN9xwQ7J+//33J+s33nhj2VqlcfxKDh06lKzv37+/rtef6tjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPNPcUuWLEnWe3t7k/V77703WZ8zZ86ke6rW+fPnk/VSqZSsj46O5tnOlMOeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2bzJW2TNEfSqKR+d/+hmbVJ2iFpoaSjkh5w9w8b12pclcbSu7u7y9YqjeMvXLiwlpZyMTg4mKyvX78+Wd+1a1ee7YRTzZ5/RNK/uftXJX1d0hozu07So5JecvdrJL2UPQZwiagYfncvufuB7P5pSYclzZO0UtLWbLGtku5pVJMA8jepc34zWyjpa5IGJM1295I09gdC0tV5Nwegcaq+tt/MvijpOUnfc/dTZlVNByYz65HUU1t7ABqlqj2/mX1BY8H/mbs/nz193MzmZvW5kk5MtK6797t7h7t35NEwgHxUDL+N7eJ/Iumwu28aV9olaVV2f5Wknfm3B6BRKk7RbWbLJf1G0tsaG+qTpHUaO+//paQFkv4s6VvufrLCa4Wconv27NnJ+nXXXZesP/3008n6tddeO+me8jIwMJCsP/nkk2VrO3em9xd8Jbc21U7RXfGc391/K6nci90+maYAtA6u8AOCIvxAUIQfCIrwA0ERfiAowg8ExU93V6mtra1sra+vL7lue3t7sr5o0aKaesrDa6+9lqxv3LgxWd+zZ0+y/sknn0y6JzQHe34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCrMOP9NN92UrK9duzZZX7ZsWdnavHnzauopLx9//HHZ2ubNm5PrPvHEE8n62bNna+oJrY89PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EFWacv6urq656PQ4dOpSs7969O1kfGRlJ1lPfuR8eHk6ui7jY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUObu6QXM5kvaJmmOpFFJ/e7+QzN7TNJqSX/NFl3n7r+q8FrpjQGom7tbNctVE/65kua6+wEzmynpDUn3SHpA0hl3f6rapgg/0HjVhr/iFX7uXpJUyu6fNrPDkor96RoAdZvUOb+ZLZT0NUkD2VO9ZvZ7M9tiZleVWafHzAbNbLCuTgHkquJh/98WNPuipFclrXf3581stqQPJLmkH2js1OChCq/BYT/QYLmd80uSmX1B0m5Je9x90wT1hZJ2u/vSCq9D+IEGqzb8FQ/7zcwk/UTS4fHBzz4IvKBL0juTbRJAcar5tH+5pN9IeltjQ32StE5St6R2jR32H5X03ezDwdRrsecHGizXw/68EH6g8XI77AcwNRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCavYU3R9I+r9xj7+UPdeKWrW3Vu1Lorda5dnbP1a7YFO/z/+5jZsNuntHYQ0ktGpvrdqXRG+1Kqo3DvuBoAg/EFTR4e8vePsprdpbq/Yl0VutCumt0HN+AMUpes8PoCCFhN/M7jSzI2b2rpk9WkQP5ZjZUTN728wOFj3FWDYN2gkze2fcc21m9msz+1N2O+E0aQX19piZvZ+9dwfN7F8L6m2+mb1sZofN7A9m9kj2fKHvXaKvQt63ph/2m9k0SX+U1ClpSNLrkrrd/VBTGynDzI5K6nD3wseEzexfJJ2RtO3CbEhm9h+STrr7huwP51Xu/u8t0ttjmuTMzQ3qrdzM0t9Rge9dnjNe56GIPf8ySe+6+3vufk7SLyStLKCPlufu+yWdvOjplZK2Zve3auw/T9OV6a0luHvJ3Q9k909LujCzdKHvXaKvQhQR/nmS/jLu8ZBaa8pvl7TXzN4ws56im5nA7AszI2W3Vxfcz8UqztzcTBfNLN0y710tM17nrYjwTzSbSCsNOXzD3f9Z0l2S1mSHt6jOjyQt1tg0biVJG4tsJptZ+jlJ33P3U0X2Mt4EfRXyvhUR/iFJ88c9/rKkYwX0MSF3P5bdnpD0gsZOU1rJ8QuTpGa3Jwru52/c/bi7n3f3UUk/VoHvXTaz9HOSfubuz2dPF/7eTdRXUe9bEeF/XdI1ZvYVM5su6duSdhXQx+eY2YzsgxiZ2QxJ31TrzT68S9Kq7P4qSTsL7OXvtMrMzeVmllbB712rzXhdyEU+2VDGf0maJmmLu69vehMTMLNFGtvbS2PfePx5kb2Z2XZJt2rsW1/HJX1f0n9L+qWkBZL+LOlb7t70D97K9HarJjlzc4N6Kzez9IAKfO/ynPE6l364wg+IiSv8gKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9f/Ex0YKZYOZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that we are trying to train these handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Convolutional2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main core of the algorithm itself. Convolutional operation is very similar to matrix multiplication. It involves an nxn kernel/filter, then it convolves with a group of nxn pixel image until the end of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ConvolutionalURL](https://media.giphy.com/media/i4NjAwytgIRDW/giphy.gif \"convolutional operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things, let's assume that we use only 3x3 filters along with 1 stride and no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "    def __init__(self, filter_shape, num_filters):\n",
    "        self.filter_shape = filter_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.filters = np.random.randn(num_filters, filter_shape[0], filter_shape[0]) # random initialization\n",
    "        \n",
    "    def iterate(self, img, filter_shape):\n",
    "        height, width = img.shape\n",
    "        \n",
    "        for i in range(height - 2):\n",
    "            for j in range(width - 2):\n",
    "                output = img[i:(i+filter_shape[0]), j:(j+filter_shape[0])]\n",
    "                yield output, i, j # 'yield' keyword will return any values and continue from the last value returned\n",
    "    \n",
    "    def conv2d(self, inputs):\n",
    "        height, width = inputs.shape\n",
    "        output = np.zeros((height-2, width-2, self.num_filters)) \n",
    "        \n",
    "        for region, i, j in self.iterate(inputs, self.filter_shape):\n",
    "            output[i, j] = np.sum(region * self.filters, axis=(1,2))\n",
    "#             print(region.shape)\n",
    "#             print(self.filters.shape)\n",
    "#             print(region * self.filters)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "convolution = Convolution2D(filter_shape=(3,3), num_filters=8)\n",
    "output = convolution.conv2d(x_train[0]) # x_train = 28x28 shape\n",
    "output.shape # output should be 26x26 because of the convolution computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape of the image \"shrinks\" after the convolutional operation. Later, we will learn about <b>padding</b> and how does it solves this shrinking problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Maxpool2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers are also commonly used in the CNN. Here, we use the maxpooling layer. There are many types of pooling layer apart from maxpool for example: average pooling, min pooling, global pooling, etc. <b>This layer functions as a \"filter\" to eliminate adjacent pixel images which have similar values (since they share the same information and it becomes redundant) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MaxPoolURL](https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif \"2x2 MaxPool\")\n",
    "image taken from: https://gfycat.com/firstmediumdalmatian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to simplify things, let's assume we use 2x2 maxpool with 1 stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def iterate(self, img):\n",
    "        height, width, _ = img.shape\n",
    "        h = height//2\n",
    "        w = width//2\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                new_region = img[(i*2):(i*2+2), (j*2):(j*2+2)]\n",
    "                yield new_region, i, j\n",
    "    \n",
    "    # 2x2 maxpool\n",
    "    def pool(self, inputs):\n",
    "        height, width, num_filters = inputs.shape\n",
    "        output = np.zeros((height//2, width//2, num_filters))\n",
    "        \n",
    "        for img_region, i, j in self.iterate(inputs):\n",
    "            output[i, j] = np.max(img_region, axis=(0,1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "convolution = Convolution2D(filter_shape=(3,3), num_filters=8)\n",
    "maxpool = MaxPool2D()\n",
    "\n",
    "output = convolution.conv2d(x_train[0]) # x_train = 28x28 shape\n",
    "output = maxpool.pool(output) # 26x26x8 before pooling\n",
    "output.shape # should be 13x13x8 after pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image dimension will halve after the pooling operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the neural network part. Remember that we are trying to classify 10 different classes of handwritten digits. So, this <b> is not a binary classification problem</b> instead, this is a <b> multiclassification problem</b>. In CNN, we call it Softmax layer which is a fully connected layer with a softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DenseURL](https://miro.medium.com/max/2808/1*SGPGG7oeSvVlV5sOSQ2iZw.png \"Fully connected layer\")\n",
    "image taken from: https://towardsdatascience.com/mnist-handwritten-digits-classification-using-a-convolutional-neural-network-cnn-af5fafbc35e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the last maxpooling layer. The image has 3 dimensions (height x width x channel) while neural network requires us to have a 2D (nx1) vector image. Finally, the last layer will be connected with 10 different nodes with its probabilities for each number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    # initialize random weights and zero biases\n",
    "    def __init__(self, num_features, num_nodes):\n",
    "        self.weights = np.random.randn(num_features, num_nodes) / num_features\n",
    "        self.biases = np.zeros(num_nodes)\n",
    "#         print(self.weights.shape)\n",
    "#         print(self.biases.shape)\n",
    "    \n",
    "    # flattens out the previous layer to nx1 vector\n",
    "    def flatten(self, inputs):\n",
    "        self.inputs = inputs.flatten()\n",
    "#         print(self.inputs.shape)\n",
    "    \n",
    "    # connects flattened layer with a fully connected layer (dense)\n",
    "    def dense(self, inputs):\n",
    "        input_features, nodes = self.weights.shape\n",
    "        \n",
    "        z = np.dot(self.inputs, self.weights) + self.biases # z = W . X + b\n",
    "        a = np.exp(z) # a = g(z)\n",
    "\n",
    "        return a / np.sum(a, axis=0) # e^a / sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might ask: <b>Why we have to convert these outputs into probabilities? Why we have to bother with other digits which have low probabilities? Just pick the digit with the highest probability</b><br><br>\n",
    "Softmax activation function <b> evaluates how sure the model is with their predictions. </b> You might heard of <b> cross-entropy loss</b> and this is exactly what we will going to use in calculating our loss between the predictions and actual labels\n",
    "<br><br>\n",
    "\n",
    "\\begin{equation*}\n",
    "s(x_i) = \\frac{e^{x_i}} {\\sum_{j=1}^n e^{x_j}}\n",
    "\\end{equation*}<br>\n",
    "\n",
    "As stated above, softmax function converts values into probabilities. It uses e (mathematical constant) for each class and divide by sum of all exponentials (powers of e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "convolution = Convolution2D(filter_shape=(3,3), num_filters=8)\n",
    "maxpool = MaxPool2D()\n",
    "softmax = Softmax(13*13*8, 10) # 13x13 input to softmax from maxpool's output --> 26x26 original size\n",
    "\n",
    "output = convolution.conv2d(x_train[0]) \n",
    "output = maxpool.pool(output)\n",
    "output = softmax.flatten(output)\n",
    "output = softmax.dense(output)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 10 outputs for each probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the layers together and try to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution = Convolution2D(filter_shape=(3,3), num_filters=8)\n",
    "maxpool = MaxPool2D()\n",
    "softmax = Softmax(13*13*8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how close our predictions are with the actual label, we need a loss function. Softmax loss function uses <b> natural logarithm</b> a.k.a <b>cross-entropy loss</b>\n",
    "\n",
    "\\begin{equation*}\n",
    "L = -ln(probability)\n",
    "\\end{equation*}<br>\n",
    "\n",
    "ln(probability) is a natural logarithm for the predicted probability for each class. The rule of thumb is, <b>lower loss leads to a better prediction</b>\n",
    "\n",
    "\\begin{equation*}\n",
    "L = -ln(1) = 0\n",
    "\\end{equation*}<br>\n",
    "\\begin{equation*}\n",
    "L = -ln(0.8) = 0.22\n",
    "\\end{equation*}<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(img, label):\n",
    "    output = convolution.conv2d(img/255)\n",
    "    output = maxpool.pool(output)\n",
    "    output = softmax.flatten(output)\n",
    "    output = softmax.dense(output)\n",
    "    \n",
    "    loss = -np.log(output[label]) # -ln(x) --> softmax loss function\n",
    "    acc = 1 if np.argmax(output) == label else 0 # increase the accuracy if the predicted label == actual label\n",
    "    \n",
    "    return output, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the data in SGD (Stochastic Gradient Descent) fashion, means the training takes one image at a team for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "accuracy = 0\n",
    "\n",
    "for i, (img, label) in enumerate(zip(x_train[:1000], y_train[:1000])): # let's train first 1000 data for simplicity \n",
    "    _, loss, acc = forward_propagation(img, label)\n",
    "    total_loss += loss\n",
    "    accuracy += acc\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch\", i, \": Loss= \", total_loss/100, \"| Accuracy=\", accuracy, \"%\")\n",
    "        \n",
    "        total_loss = 0\n",
    "        accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Notice that our loss is stuck around ~2.3. This is because there is no gradient updates a.k.a backpropagation! Our model did not learn anything during this training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding is useful when we want to keep the resolution of the image (does not shrink after the convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PaddingURL](https://jamesmccaffrey.files.wordpress.com/2018/05/convolution_math.jpg \"Image Padding\")\n",
    "Image taken from: https://jamesmccaffrey.wordpress.com/2018/05/30/convolution-image-size-filter-size-padding-and-stride/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(img, pad):\n",
    "    img_pad = np.pad(img, ((pad, pad), (pad,pad)), 'constant', constant_values=0)\n",
    "    \n",
    "    return img_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (28, 28)\n",
      "x_pad.shape = (30, 30)\n",
      "x[1,1] = [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253\n",
      "  159  50   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252\n",
      "  252 237   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239\n",
      "  233 252  57   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202\n",
      "   84 252 253 122   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252\n",
      "   96 189 253 167   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "   47  79 255 168   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21\n",
      "    0   0 253 243  50   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0\n",
      "    0   0 253 252 165   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0\n",
      "    0   0 255 253 196   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0\n",
      "    0   0 253 252 148   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0\n",
      "    7 135 253 186  12   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7\n",
      "  131 252 225  71   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      "  252 173   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253\n",
      "  162   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167\n",
      "   56   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "x_pad[1,1] = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27ca0daa320>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pad = padding(x_train[0], 1)\n",
    "print (\"x.shape =\", x_train[0].shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x_train[1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x_train[0,:,:], cmap='gray')\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'x_pad' image looks kind of zoomed out than the original image 'x', this is due to the zero padding that adds more pixel density to the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement it in the Convolution2D class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "    def __init__(self, filter_shape, num_filters, padding, stride, debugging=False):\n",
    "        self.filter_shape = filter_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filters = np.random.randn(num_filters, filter_shape[0], filter_shape[0]) # random initialization\n",
    "        self.debugging = debugging\n",
    "        \n",
    "    def iterate(self, img, filter_shape):\n",
    "        height, width = img.shape\n",
    "        height = int((height + 2 * self.pad_size - self.filter_shape[0]) / self.stride) + 1\n",
    "        width = int((width + 2 * self.pad_size - self.filter_shape[0]) / self.stride) + 1\n",
    "        # check for odd heights and widths\n",
    "        if height % 2 != 0 and width % 2 != 0:\n",
    "            height += 1\n",
    "            width += 1\n",
    "#         print(height)\n",
    "#         print(width)\n",
    "#         print(filter_shape[0]-1)\n",
    "        \n",
    "        for i in range(height-(filter_shape[0]-1)):\n",
    "            for j in range(width-(filter_shape[0]-1)):\n",
    "#                 i_start = i * self.stride\n",
    "#                 i_end = i_start + filter_shape[0]\n",
    "#                 j_start = j * self.stride\n",
    "#                 j_end = j_start * filter_shape[0]\n",
    "#                 output = img[i_start:i_end, j_start:j_end]\n",
    "                output = img[i*self.stride:(i*self.stride+filter_shape[0]), j*self.stride:(j*self.stride+filter_shape[0])]\n",
    "#                 print(output, i, j)\n",
    "                yield output, i, j # 'yield' keyword will return any values and continue from the last value returned\n",
    "    \n",
    "    def conv2d(self, inputs):\n",
    "        if self.debugging==True: print(\"Before padding: \", inputs.shape)\n",
    "    \n",
    "        # padding\n",
    "        if(self.padding.lower() == 'same'): # same padding\n",
    "            height, width = inputs.shape\n",
    "            \n",
    "            pad_size = int(((height * self.stride) - height + self.filter_shape[0] - 1) / 2)\n",
    "            self.pad_size = pad_size\n",
    "\n",
    "            inputs = padding(inputs, pad_size) # apply padding according to the pad_size\n",
    "            height, width = inputs.shape # reinitialize height and width with padded image\n",
    "\n",
    "            new_height = int((height + 2 * pad_size - self.filter_shape[0]) / self.stride) + 1\n",
    "            new_width = int((width + 2 * pad_size - self.filter_shape[0]) / self.stride) + 1\n",
    "            # check for odd heights and widths\n",
    "            if new_height % 2 != 0 and new_width % 2 != 0:\n",
    "                new_height += 1\n",
    "                new_width += 1\n",
    "                \n",
    "            output = np.zeros((new_height, new_width, self.num_filters))\n",
    "            \n",
    "        elif(self.padding.lower() == 'valid'): # valid/no padding\n",
    "            height, width = inputs.shape\n",
    "            self.pad_size = 0\n",
    "            output = np.zeros((height-(self.filter_shape[0]-1), width-(self.filter_shape[0]-1), self.num_filters))\n",
    "            \n",
    "        if self.debugging==True: print(\"After padding: \", inputs.shape)\n",
    "        \n",
    "        for region, i, j in self.iterate(inputs, self.filter_shape):\n",
    "            output[i, j] = np.sum(region * self.filters, axis=(1,2))\n",
    "        \n",
    "        if self.debugging==True: print(\"After convolution: \", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding:  (28, 28)\n",
      "After padding:  (32, 32)\n",
      "After convolution:  (32, 32, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 32, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "convolution = Convolution2D(filter_shape=(5,5), num_filters=8, padding='same', stride=1, debugging=True)\n",
    "output = convolution.conv2d(x_train[0]) # x_train = nxn shape\n",
    "output.shape # output should be the same after convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding:  (28, 28)\n",
      "After padding:  (28, 28)\n",
      "After convolution:  (23, 23, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23, 23, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "convolution = Convolution2D(filter_shape=(6,6), num_filters=8, stride=1, padding='valid')\n",
    "output = convolution.conv2d(x_train[0]) # x_train = nxn shape\n",
    "output.shape # output should be nxn because of padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can use padding by adding a parameter \"padding\" when calling the Convolution2D class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Back Propagation (Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before when we tried to train a model that only has forward propagation, it did not perform well as it did not learn anything from the training phase. This is because our model only knows how to calculate and output predictions without the ability to trace back (calculating gradients and update weights & biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BackpropURL](https://miro.medium.com/max/1742/1*FceBJSJ7j8jHjb4TmLV0Ew.png \"Backpropagation with its derivative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the illustration above, backpropagation starts from the very last layer of our convolutional model, which is the softmax layer. The gradient is simply the partial derivation from loss over the partial derivation from the activation function from previous layer\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial z} = 0 if {i \\neq c}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial z} = \\frac{-1}{p_i} if {i = c}\n",
    "\\end{equation*}\n",
    "\n",
    "The partial derivation of this equation is simple:\n",
    "1. 0 if the prediction is not the same with the label\n",
    "2. -1/pi if the prediction is the same as the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed the Softmax backpropagation phase. Now, let's calculate the gradient from activation function z.<br>\n",
    "<br>Recall that Softmax activation function is\n",
    "\\begin{equation*}\n",
    "S = {\\sum_{i} e^{t_i}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "z = \\frac{e^{x_i}} {\\sum_{j=1}^n e^{x_j}}\n",
    "\\end{equation*}\n",
    "\n",
    "We can substitute the denominator in z\n",
    "\\begin{equation*}\n",
    "z = \\frac{e^{x_i}} {S} = {e^{x_i}} {S^{-1}}\n",
    "\\end{equation*}\n",
    "\n",
    "Apply the chain rule to obtain the derivative\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial z}{\\partial {t_k}} = \\frac{{-e^{t_c}}{e^{t_k}}}{S^2} \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial z}{\\partial t_c} = \\frac{{e^{t_c}}{(S - e^{t_c})}}{S^2} \n",
    "\\end{equation*}\n",
    "\n",
    "Alright, let's implement them in the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    # initialize random weights and zero biases\n",
    "    def __init__(self, num_features, num_nodes):\n",
    "        self.weights = np.random.randn(num_features, num_nodes) / num_features\n",
    "        self.biases = np.zeros(num_nodes)\n",
    "#         print(self.weights.shape)\n",
    "#         print(self.biases.shape)\n",
    "    \n",
    "    # move flatten inside the dense function\n",
    "#     def flatten(self, inputs):\n",
    "#         self.inputs = inputs.flatten()\n",
    "#         print(self.inputs.shape)\n",
    "    \n",
    "    # connects flattened layer with a fully connected layer (dense)\n",
    "    def dense(self, inputs):\n",
    "        self.last_input_shape = inputs.shape # cache the last input shape BEFORE FLATTENING\n",
    "        \n",
    "        inputs = inputs.flatten()\n",
    "        self.last_input = inputs # cache the last input shape AFTER FLATTENING\n",
    "        input_features, nodes = self.weights.shape\n",
    "        \n",
    "        z = np.dot(inputs, self.weights) + self.biases # z = W . X + b\n",
    "        self.z = z # cache z for backpropagation\n",
    "        a = np.exp(z) # a = g(z)\n",
    "        \n",
    "        return a / np.sum(a, axis=0) # e^a / sum(a)\n",
    "    \n",
    "    def back_propagation(self, dL, learning_rate):\n",
    "        for i, grad in enumerate(dL):\n",
    "            if grad == 0: continue; # ignores 0 gradient\n",
    "            \n",
    "            exp_total = np.exp(self.z) # total of e^\n",
    "            exp_sum = np.sum(exp_total) # sum of e^\n",
    "            \n",
    "            # gradients of z against totals\n",
    "            dz = -exp_total[i] * exp_total / (exp_sum ** 2)\n",
    "            dz[i] = exp_total[i] * (exp_sum - exp_total[i]) / (exp_sum ** 2)\n",
    "            \n",
    "            # gradients of totals against weights, biases, inputs\n",
    "            dt_dw = self.last_input\n",
    "            dt_db = 1\n",
    "            dt_di = self.weights\n",
    "            \n",
    "            # gradients of loss against totals\n",
    "            dL_dt = grad * dz\n",
    "            \n",
    "            # gradients of loss against weights, biases, and inputs\n",
    "            dL_dw = np.dot(dt_dw[np.newaxis].T, dL_dt[np.newaxis])\n",
    "            dL_db = dL_dt * dt_db\n",
    "            dL_di = np.dot(dt_di, dL_dt)\n",
    "            \n",
    "            # update weights and biases\n",
    "            self.weights -= learning_rate * dL_dw\n",
    "            self.biases -= learning_rate * dL_db\n",
    "            \n",
    "            return dL_di.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution = Convolution2D(filter_shape=(3,3), num_filters=8, padding='same', stride=1)\n",
    "maxpool = MaxPool2D()\n",
    "softmax = Softmax(15*15*8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(img, label):\n",
    "    output = convolution.conv2d(img/255)\n",
    "    output = maxpool.pool(output)\n",
    "#     output = softmax.flatten(output)\n",
    "    output = softmax.dense(output)\n",
    "    \n",
    "    loss = -np.log(output[label]) # -log(x) --> softmax loss function\n",
    "    acc = 1 if np.argmax(output) == label else 0 # increase the accuracy if the predicted label = actual label\n",
    "    \n",
    "    return output, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(img, label, learning_rate):\n",
    "    # forward propagation\n",
    "    output, loss, acc = forward_propagation(img, label)\n",
    "    \n",
    "    # initial gradient\n",
    "    grad = np.zeros(10) # 10 different classes\n",
    "    grad[label] = -1 / output[label]\n",
    "    \n",
    "    # back propagation\n",
    "    grad = softmax.back_propagation(grad, learning_rate)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0: Loss=0.000 | Accuracy=0.00%\n",
      "Epoch #100: Loss=2.357 | Accuracy=43.00%\n",
      "Epoch #200: Loss=1.320 | Accuracy=68.00%\n",
      "Epoch #300: Loss=1.313 | Accuracy=70.00%\n",
      "Epoch #400: Loss=0.948 | Accuracy=75.00%\n",
      "Epoch #500: Loss=0.817 | Accuracy=78.00%\n",
      "Epoch #600: Loss=1.274 | Accuracy=73.00%\n",
      "Epoch #700: Loss=1.341 | Accuracy=77.00%\n",
      "Epoch #800: Loss=0.750 | Accuracy=79.00%\n",
      "Epoch #900: Loss=1.624 | Accuracy=73.00%\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "acc = 0\n",
    "\n",
    "for i, (img, label) in enumerate(zip(x_train[:1000], y_train[:1000])):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch #%d: Loss=%.3f | Accuracy=%.2f%%\" % (i, l/100, acc))\n",
    "        l = 0\n",
    "        acc = 0\n",
    "    loss, accuracy = train(img, label, learning_rate=0.005)\n",
    "    l += loss\n",
    "    acc += accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great it's working! Now you can see that the accuracy is gradually increasing each epoch. But this is still incomplete because we only backpropagate through the Softmax layer, we also have to backpropagate through the MaxPooling and Convolution2D layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Back Propagation (MaxPool & Conv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward propagation phase, pooling layer halves the dimension by extracting a maximum value over 2x2 blocks. Now backpropagation does the opposite: <b>double the dimension of the loss gradient by assigning each gradient value to where the original max value was</b> in its 2x2 block\n",
    "\n",
    "![PoolBackpropURL](https://victorzhou.com/media/cnn-post/maxpool-forward.svg \"2x2 MaxPooling Backpropagation\")\n",
    "Image taken from: https://victorzhou.com/blog/intro-to-cnns-part-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def iterate(self, img):\n",
    "        height, width, _ = img.shape\n",
    "        h = height//2\n",
    "        w = width//2\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                new_region = img[(i*2):(i*2+2), (j*2):(j*2+2)]\n",
    "                yield new_region, i, j\n",
    "    \n",
    "    # 2x2 maxpool --> nxn maxpool if possible\n",
    "    def pool(self, inputs):\n",
    "        self.last_input = inputs # cache last input for backpropagation\n",
    "        \n",
    "        height, width, num_filters = inputs.shape\n",
    "        output = np.zeros((height//2, width//2, num_filters))\n",
    "        \n",
    "        for img_region, i, j in self.iterate(inputs):\n",
    "            output[i, j] = np.max(img_region, axis=(0,1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def back_propagation(self, dL_output):\n",
    "        dL_input = np.zeros((self.last_input.shape))\n",
    "        \n",
    "        for img_region, i, j in self.iterate(self.last_input):\n",
    "            height, width, num_filters = img_region.shape\n",
    "            # find the max value for each region\n",
    "            maxi = np.max(img_region, axis=(0,1))\n",
    "            \n",
    "            for k in range(height):\n",
    "                for l in range(width):\n",
    "                    for m in range(num_filters):\n",
    "                        if img_region[k, l, m] == maxi[m]: # if the max values match, copy the gradient\n",
    "                            dL_input[i*2+k, j*2+l, m] = dL_output[i, j, m]\n",
    "        return dL_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backpropagation on Convolution 2D, we are only interested in <b>the loss gradient for the filters in the convolutional layers</b> to update filter weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "    def __init__(self, filter_shape, num_filters):\n",
    "        self.filter_shape = filter_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.filters = np.random.randn(num_filters, filter_shape[0], filter_shape[0]) # random initialization\n",
    "        \n",
    "    def iterate(self, img, filter_shape):\n",
    "        height, width = img.shape\n",
    "        \n",
    "        for i in range(height - 2):\n",
    "            for j in range(width - 2):\n",
    "                output = img[i:(i+filter_shape[0]), j:(j+filter_shape[0])]\n",
    "                yield output, i, j # 'yield' keyword will return any values and continue from the last value returned\n",
    "    \n",
    "    def conv2d(self, inputs):\n",
    "        self.last_input = inputs # cache the last input for backpropagation\n",
    "        \n",
    "        height, width = inputs.shape\n",
    "        output = np.zeros((height-2, width-2, self.num_filters)) \n",
    "        \n",
    "        for region, i, j in self.iterate(inputs, self.filter_shape):\n",
    "            output[i, j] = np.sum(region * self.filters, axis=(1,2))\n",
    "#             print(region * self.filters)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def back_propagation(self, dL, learning_rate):\n",
    "        dL_filters = np.zeros(self.filters.shape)\n",
    "        \n",
    "        for img_region, i, j in self.iterate(self.last_input, self.filter_shape):\n",
    "            for k in range(self.num_filters):\n",
    "                dL_filters[k] += dL[i, j, k] * img_region\n",
    "        \n",
    "        # don't forget to update the filters\n",
    "        self.filters -= learning_rate * dL_filters\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Putting it altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the basic construction for our model is complete! Let's put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Convolution2D(num_filters=8, filter_shape=(3,3))\n",
    "pooling = MaxPool2D()\n",
    "softmax = Softmax(13*13*8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(img, label):\n",
    "    # define the model\n",
    "    model = conv.conv2d(img/255)\n",
    "    model = pooling.pool(model)\n",
    "    model = softmax.dense(model)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = -np.log(model[label])\n",
    "    acc = 1 if np.argmax(model) == label else 0\n",
    "    \n",
    "    return model, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(img, label, learning_rate=0.005):\n",
    "    # forward propagation\n",
    "    output, loss, acc = forward_propagation(img, label) # test with 1000 images\n",
    "    # initial gradient\n",
    "    grad = np.zeros(10)\n",
    "    grad[label] = -1 / output[label]\n",
    "    # back propagation\n",
    "    grad = softmax.back_propagation(grad, learning_rate)\n",
    "#     print(grad.shape)\n",
    "    grad = pooling.back_propagation(grad)\n",
    "    grad = conv.back_propagation(grad, learning_rate)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH # 0\n",
      "Step #0: Loss=0.000 | Accuracy=0 %\n",
      "Step #100: Loss=1.950 | Accuracy=45 %\n",
      "Step #200: Loss=1.168 | Accuracy=67 %\n",
      "Step #300: Loss=0.956 | Accuracy=71 %\n",
      "Step #400: Loss=0.709 | Accuracy=81 %\n",
      "Step #500: Loss=0.613 | Accuracy=81 %\n",
      "Step #600: Loss=0.902 | Accuracy=75 %\n",
      "Step #700: Loss=0.859 | Accuracy=80 %\n",
      "Step #800: Loss=0.507 | Accuracy=84 %\n",
      "Step #900: Loss=0.894 | Accuracy=80 %\n",
      "EPOCH # 1\n",
      "Step #0: Loss=0.000 | Accuracy=0 %\n",
      "Step #100: Loss=0.360 | Accuracy=89 %\n",
      "Step #200: Loss=0.454 | Accuracy=86 %\n",
      "Step #300: Loss=0.490 | Accuracy=85 %\n",
      "Step #400: Loss=0.235 | Accuracy=93 %\n",
      "Step #500: Loss=0.346 | Accuracy=87 %\n",
      "Step #600: Loss=0.337 | Accuracy=90 %\n",
      "Step #700: Loss=0.405 | Accuracy=86 %\n",
      "Step #800: Loss=0.282 | Accuracy=92 %\n",
      "Step #900: Loss=0.395 | Accuracy=88 %\n",
      "EPOCH # 2\n",
      "Step #0: Loss=0.000 | Accuracy=0 %\n",
      "Step #100: Loss=0.198 | Accuracy=95 %\n",
      "Step #200: Loss=0.188 | Accuracy=93 %\n",
      "Step #300: Loss=0.303 | Accuracy=90 %\n",
      "Step #400: Loss=0.091 | Accuracy=99 %\n",
      "Step #500: Loss=0.202 | Accuracy=91 %\n",
      "Step #600: Loss=0.167 | Accuracy=95 %\n",
      "Step #700: Loss=0.222 | Accuracy=95 %\n",
      "Step #800: Loss=0.193 | Accuracy=94 %\n",
      "Step #900: Loss=0.242 | Accuracy=95 %\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for epoch in range(3):\n",
    "    print(\"EPOCH #\", epoch)\n",
    "    \n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for i, (img, label) in enumerate(zip(x_train[:1000], y_train[:1000])):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Step #%d: Loss=%.3f | Accuracy=%d %%\" % (i, loss/100, accuracy))\n",
    "            loss = 0\n",
    "            accuracy = 0\n",
    "            \n",
    "        l, acc = train(img, label)\n",
    "        loss += l\n",
    "        accuracy += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.6366723476818364\n",
      "Test Accuracy:  81.39999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "for img, label in zip(x_test[:1000], y_test[:1000]):\n",
    "    _, l, acc = forward_propagation(img, label)\n",
    "    loss += l\n",
    "    accuracy += acc\n",
    "            \n",
    "print(\"Test Loss: \", loss/len(x_test[:1000]))\n",
    "print(\"Test Accuracy: \", accuracy/len(x_test[:1000])* 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8 - Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our model with keras built-in library model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# reshape\n",
    "x_train = np.reshape(x_train, ((60000,28,28,1)))\n",
    "x_test = np.reshape(x_test, (10000,28, 28,1))\n",
    "\n",
    "# one-hot encoding for label y\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 15,770\n",
      "Trainable params: 15,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model with the same hyperparameters as the scratch model\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Conv2D(filters=8, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "keras_model.add(MaxPool2D(pool_size=2))\n",
    "\n",
    "keras_model.add(Flatten())\n",
    "keras_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = optimizers.SGD(lr=0.01)\n",
    "keras_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) # optimizer is not yet implemented\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 669 samples, validate on 331 samples\n",
      "Epoch 1/3\n",
      "669/669 [==============================] - 6s 9ms/step - loss: 1.2066 - acc: 0.6054 - val_loss: 0.6571 - val_acc: 0.7946\n",
      "Epoch 2/3\n",
      "669/669 [==============================] - 5s 8ms/step - loss: 0.4856 - acc: 0.8460 - val_loss: 0.5887 - val_acc: 0.8338\n",
      "Epoch 3/3\n",
      "669/669 [==============================] - 6s 10ms/step - loss: 0.3171 - acc: 0.8969 - val_loss: 0.6336 - val_acc: 0.8248\n"
     ]
    }
   ],
   "source": [
    "history = keras_model.fit(x_train[:1000], y_train[:1000], validation_split=0.33, batch_size=1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 273us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5380505723953247, 0.822]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.evaluate(x_test[:1000], y_test[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! It is similar with our scratch model achieving ~80% accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
